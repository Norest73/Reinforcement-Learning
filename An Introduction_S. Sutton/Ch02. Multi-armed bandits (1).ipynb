{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch02. Multi-armed bandits\n",
    "\n",
    "### Intro.\n",
    "\n",
    "  - Evaluative feedback과 Purely instructive feedback의 차이에 대해 서술\n",
    "  - RL은 전체 actions들에 대해 더 좋은 선택을 하도록 지속적으로 평가해나감\n",
    "  - ML, DL은 대부분 Instructive feedback으로 특정 행동을 지시하는게 초점을 맞춤\n",
    "\n",
    "### 2.1 A k-armed Bandit Problem\n",
    "\n",
    "  - K 개의 options중 하나의 action을 선택하는 문제 (K개의 팔을 가진슬롯머신)\n",
    "  - action들은 각각의 Stationary probability distribution을 가짐\n",
    "  - objective는 일정 기간동안 expected total reward를 최대화 하는 것\n",
    "  \n",
    "$$ q_{*}(a) = E [R_{t} | A_{t} = a] $$\n",
    "\n",
    "  - $q_{*}(a), Q_{t}(a)$ : Value of an action a on time step t\n",
    "  - $R_{t}$ : Reward on time step t\n",
    "  - $A_{t}$ : Action on time step t\n",
    "  - $greedy~actions$ : 현 상황에서 Estimated value가 가장 높은 값을 가진 actions\n",
    "  - $exploiting$ : 각 상황에서 Estimated value가 가장 높은 greedy action을 취하는 것\n",
    "  - $exploring$ : non greedy action을 통해 더 value가 높은 action을 찾는 것\n",
    "  - 두 선택지 사이에서 balance를 찾는 것이 중점적으로 다루어지는 문제\n",
    "  \n",
    "### 2.2 Action-value Methods\n",
    "\n",
    "  - action selection을 하기 위해서는 action-value를 예측하는 방법을 정해야한다.\n",
    "  - action-value를 예측하는 가장 간단한 방법은 평균을 산출하는 것이다.\n",
    "  - $ Q_{t}(a) $ : sum of rewards when a taken prior to t / number of times a taken prior to t\n",
    "  - 가장 간단한 action selection은 greedy action selection이다 (estimated value 기준 선택)\n",
    "  - $ A_{t} = argmax_{a}Q_{t}(a)$ :  ($Q_{t}(a)$를 최대화시키는 action a를 반환)\n",
    "  - $ \\epsilon-greedy method $ : $\\epsilon$  확률로 exploring (non-greedy action)을 실시\n",
    "\n",
    "### 2.3 The 10-armed Testbed\n",
    "\n",
    "  - greedy method와 $\\epsilon$-greedy method 성능 비교를 위해 실험을 진행\n",
    "  - 2000개의 10-armed bandit 문제를 랜덤하게 생성하여 진행\n",
    "  \n",
    "![2.3.1](./imgs/2.3.1.jpg)\n",
    "\n",
    "  - 10개의 각 action들에 대한 optimal action value, $q_{*}(a)$의 값을 mean=0, var=1인 normal distribution으로 생성\n",
    "  - 각 실험을 $\\epsilon$ = 0.1, 0.01, 0 (greedy)에 대해 각각 실시\n",
    "  \n",
    "![2.3.2](./imgs/2.3.2.jpg)\n",
    "\n",
    "  - greedy method는 처음에는 빠르게 향상되지만 low level에 수렴\n",
    "  - $\\epsilon$-greedy method는 처음엔 느리지만 지속 발전해나가며 high level에 수렴\n",
    "  - 절대적으로 좋은 방법은 없으며 각 상황에따라 적절한 방법을 선택하는 것이 좋음\n",
    "  \n",
    "### 2.4 Incremental Implementation\n",
    "\n",
    "  - action value를 추정하는 sample average를 좀 더 효율적으로 계산하는 방법에 대해 소개\n",
    "\n",
    "$$ Q_{n} = \\frac{R_{1}+R_{2}+ ... + R_{n-1}}{n-1} $$\n",
    "\n",
    "  - 위와 같이 계산하기 위해서는 2가지 문제점이 있다.\n",
    "  - 모든 time step마다 발생된 reward를 기록해야 하며, 매번 reward의 합을 구해야한다\n",
    "  \n",
    "$$ Q_{n+1} = Q_{n} + \\frac{1}{n}[R_{n}-Q_{n}] $$\n",
    "\n",
    "  - 위와 같이 유도하게 되면 $Q_{n}$과 time step $n$에 대한 추가정보 만으로 $Q_{n+1}$을 산출할 수 있다\n",
    "  \n",
    "![2.4.1](./imgs/2.4.1.jpg)\n",
    "\n",
    "### 2.5 Tracking a Nonstationary Problem\n",
    "\n",
    "  - 지금까지의 예제와 달리 확률분포가 유동적인 Nonstationary problems가 실제로는 더 많다.\n",
    "  - 확률분포가 유동적이라면, 먼 과거보다 최근에 받은 Reward에 가중치를 줘야 한다\n",
    "  \n",
    "![2.5.1](./imgs/2.5.1.jpg)\n",
    "\n",
    "  - $ (1-a)^n + \\sum_{i=1}^{n}a(1-a)^{n-i} = 1 $ 이므로 이는 weighted average와 같다\n",
    "  - $ \\sum_{i=1}^{n}a(1-a)^{n-i}R_{t} $는 오래된 reward일수록 낮은 가중치가 곱해지는 걸 의미"
   ]
  },
 ]
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
